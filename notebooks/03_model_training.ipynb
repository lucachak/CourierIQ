
# %% [markdown]
# # CourierIQ ETA Model Training
#
# This notebook walks through:
# 1. Data loading and exploration
# 2. Feature engineering
# 3. Model training (multiple algorithms)
# 4. Model evaluation
# 5. Model saving

# %% Import libraries
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path.cwd().parent))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

from src.models.eta_model import ETAPredictor, generate_synthetic_training_data

# Set style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

# %% [markdown]
# ## 1. Load or Generate Data

# %% Load data
# Option 1: Load real data (if available)
# data = pd.read_csv('../src/data/processed/delivery_data.csv')

# Option 2: Generate synthetic data for testing
print("Generating synthetic training data...")
data = generate_synthetic_training_data(n_samples=5000)

print(f"Dataset shape: {data.shape}")
print(f"\nColumns: {data.columns.tolist()}")
print(f"\nFirst few rows:")
data.head()

# %% Data exploration
print("\n=== Data Summary ===")
print(data.describe())

print(f"\n=== Missing Values ===")
print(data.isnull().sum())

print(f"\n=== Target Variable (ETA) ===")
print(f"Mean ETA: {data['eta_seconds'].mean()/60:.1f} minutes")
print(f"Median ETA: {data['eta_seconds'].median()/60:.1f} minutes")
print(f"Std ETA: {data['eta_seconds'].std()/60:.1f} minutes")

# %% Visualize ETA distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(data['eta_seconds'] / 60, bins=50, edgecolor='black')
axes[0].set_xlabel('ETA (minutes)')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Distribution of ETAs')

axes[1].boxplot(data['eta_seconds'] / 60)
axes[1].set_ylabel('ETA (minutes)')
axes[1].set_title('ETA Boxplot')

plt.tight_layout()
plt.show()

# %% Visualize relationships
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ETA vs number of stops
axes[0, 0].scatter(data['num_stops'], data['eta_seconds'] / 60, alpha=0.3)
axes[0, 0].set_xlabel('Number of Stops')
axes[0, 0].set_ylabel('ETA (minutes)')
axes[0, 0].set_title('ETA vs Number of Stops')

# ETA vs traffic level
axes[0, 1].scatter(data['traffic_level'], data['eta_seconds'] / 60, alpha=0.3)
axes[0, 1].set_xlabel('Traffic Level')
axes[0, 1].set_ylabel('ETA (minutes)')
axes[0, 1].set_title('ETA vs Traffic Level')

# ETA by hour of day
hourly_eta = data.groupby(data['timestamp'].dt.hour)['eta_seconds'].mean() / 60
axes[1, 0].plot(hourly_eta.index, hourly_eta.values, marker='o')
axes[1, 0].set_xlabel('Hour of Day')
axes[1, 0].set_ylabel('Average ETA (minutes)')
axes[1, 0].set_title('Average ETA by Hour')
axes[1, 0].grid(True)

# ETA by day of week
daily_eta = data.groupby(data['timestamp'].dt.dayofweek)['eta_seconds'].mean() / 60
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
axes[1, 1].bar(range(7), daily_eta.values)
axes[1, 1].set_xticks(range(7))
axes[1, 1].set_xticklabels(days)
axes[1, 1].set_ylabel('Average ETA (minutes)')
axes[1, 1].set_title('Average ETA by Day of Week')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 2. Train Models
#
# We'll train multiple model types and compare them

# %% Train Random Forest
print("\n" + "="*60)
print("Training Random Forest Model")
print("="*60)

rf_predictor = ETAPredictor(model_type='random_forest')
rf_metrics = rf_predictor.train(data, target_column='eta_seconds', validate=True)

print("\nRandom Forest Results:")
for metric, value in rf_metrics.items():
    print(f"  {metric}: {value:.2f}")

# %% Train Gradient Boosting
print("\n" + "="*60)
print("Training Gradient Boosting Model")
print("="*60)

gb_predictor = ETAPredictor(model_type='gradient_boosting')
gb_metrics = gb_predictor.train(data, target_column='eta_seconds', validate=True)

print("\nGradient Boosting Results:")
for metric, value in gb_metrics.items():
    print(f"  {metric}: {value:.2f}")

# %% Train Neural Network
print("\n" + "="*60)
print("Training Neural Network Model")
print("="*60)

nn_predictor = ETAPredictor(model_type='neural_net')
nn_metrics = nn_predictor.train(data, target_column='eta_seconds', validate=True)

print("\nNeural Network Results:")
for metric, value in nn_metrics.items():
    print(f"  {metric}: {value:.2f}")

# %% Compare models
comparison = pd.DataFrame({
    'Random Forest': rf_metrics,
    'Gradient Boosting': gb_metrics,
    'Neural Network': nn_metrics
}).T

print("\n=== Model Comparison ===")
print(comparison)

# Find best model (lowest test RMSE)
best_model_name = comparison['test_rmse'].idxmin()
print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   Test RMSE: {comparison.loc[best_model_name, 'test_rmse']:.2f} seconds")
print(f"   Test MAE: {comparison.loc[best_model_name, 'test_mae']:.2f} seconds")
print(f"   Test R¬≤: {comparison.loc[best_model_name, 'test_r2']:.4f}")

# %% Visualize model comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

metrics_to_plot = ['test_mae', 'test_rmse', 'test_r2']
titles = ['Test MAE (lower is better)', 'Test RMSE (lower is better)', 'Test R¬≤ (higher is better)']

for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):
    values = comparison[metric]
    axes[idx].bar(range(len(values)), values.values)
    axes[idx].set_xticks(range(len(values)))
    axes[idx].set_xticklabels(values.index, rotation=45, ha='right')
    axes[idx].set_title(title)
    axes[idx].set_ylabel('Score')

plt.tight_layout()
plt.show()

# %% Select best model
if best_model_name == 'Random Forest':
    best_predictor = rf_predictor
elif best_model_name == 'Gradient Boosting':
    best_predictor = gb_predictor
else:
    best_predictor = nn_predictor

print(f"\nSelected {best_model_name} as final model")

# %% Feature importance (if available)
print("\n=== Feature Importance ===")
importance = best_predictor.get_feature_importance()

if not importance.empty:
    print(importance.head(15))

    # Plot top 10 features
    plt.figure(figsize=(10, 6))
    top_features = importance.head(10)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance')
    plt.title('Top 10 Most Important Features')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 3. Model Validation
#
# Test the model on sample predictions

# %% Test predictions
print("\n=== Sample Predictions ===")

# Create test scenarios
test_scenarios = pd.DataFrame({
    'origin_lat': [40.7128, 40.7580, 40.6782, 40.7614, 40.7489],
    'origin_lon': [-74.0060, -73.9855, -73.9442, -73.9776, -73.9680],
    'dest_lat': [40.7589, 40.7128, 40.7128, 40.6782, 40.7128],
    'dest_lon': [-73.9851, -74.0060, -74.0060, -73.9442, -74.0060],
    'timestamp': [datetime.now() + timedelta(hours=h) for h in range(5)],
    'num_stops': [1, 3, 5, 2, 4],
    'traffic_level': [0.3, 0.7, 0.9, 0.4, 0.6],
    'preparation_time': [600, 900, 1200, 600, 800]
})

predictions = best_predictor.predict(test_scenarios)

print("\nScenario | Predicted ETA (min)")
print("-" * 40)
for i, eta in enumerate(predictions):
    print(f"   {i+1}     |     {eta/60:.1f}")

# %% Prediction distribution analysis
print("\n=== Prediction Analysis on Full Test Set ===")

# Make predictions on a test split
from sklearn.model_selection import train_test_split

_, test_data = train_test_split(data, test_size=0.2, random_state=42)
test_predictions = best_predictor.predict(test_data)
test_actual = test_data['eta_seconds'].values

# Calculate residuals
residuals = test_actual - test_predictions

# Plot predictions vs actual
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Scatter plot
axes[0].scatter(test_actual / 60, test_predictions / 60, alpha=0.3)
axes[0].plot([0, test_actual.max()/60], [0, test_actual.max()/60], 'r--', lw=2)
axes[0].set_xlabel('Actual ETA (minutes)')
axes[0].set_ylabel('Predicted ETA (minutes)')
axes[0].set_title('Predictions vs Actual')

# Residuals
axes[1].hist(residuals / 60, bins=50, edgecolor='black')
axes[1].set_xlabel('Residual (minutes)')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Prediction Residuals')
axes[1].axvline(0, color='r', linestyle='--', lw=2)

# Error by actual ETA
bins = np.percentile(test_actual, [0, 25, 50, 75, 100])
bin_labels = ['Q1', 'Q2', 'Q3', 'Q4']
test_data_with_pred = test_data.copy()
test_data_with_pred['prediction'] = test_predictions
test_data_with_pred['bin'] = pd.cut(test_data_with_pred['eta_seconds'], bins=bins, labels=bin_labels)
error_by_bin = test_data_with_pred.groupby('bin').apply(
    lambda x: np.abs(x['eta_seconds'] - x['prediction']).mean() / 60
)

axes[2].bar(range(len(error_by_bin)), error_by_bin.values)
axes[2].set_xticks(range(len(error_by_bin)))
axes[2].set_xticklabels(error_by_bin.index)
axes[2].set_ylabel('Mean Absolute Error (minutes)')
axes[2].set_xlabel('ETA Quartile')
axes[2].set_title('Error by ETA Range')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 4. Save Model

# %% Save model
model_dir = Path('../src/models')
model_dir.mkdir(parents=True, exist_ok=True)

model_path = model_dir / 'eta_regressor.pkl'
best_predictor.save(str(model_path))

print(f"\n‚úÖ Model saved to: {model_path}")
print(f"‚úÖ Model stats saved to: {model_path.with_suffix('.json').name.replace('.pkl', '_stats.json')}")

# %% [markdown]
# ## 5. Load and Test Saved Model

# %% Load saved model
loaded_predictor = ETAPredictor.load(str(model_path))

print("\n=== Loaded Model Info ===")
print(f"Model type: {loaded_predictor.model_type}")
print(f"Number of features: {len(loaded_predictor.feature_names)}")
print(f"Training samples: {loaded_predictor.training_stats.get('train_samples', 'N/A')}")

# Test with single prediction
test_eta = loaded_predictor.predict_single(
    origin_lat=40.7128,
    origin_lon=-74.0060,
    dest_lat=40.7589,
    dest_lon=-73.9851,
    num_stops=2,
    traffic_level=0.5
)

print(f"\n‚úÖ Test prediction: {test_eta/60:.1f} minutes")

print("\n" + "="*60)
print("Training Complete! Model is ready for use.")
print("="*60)
